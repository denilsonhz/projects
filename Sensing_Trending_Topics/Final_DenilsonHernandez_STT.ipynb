{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f632a5a-527c-49e3-a53f-f35941c05dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FINAL PROJECT\n",
    "The goal of this project is to implement trending topic detection algorithms in X messages, as proposed in the article \"Sensing Trending Topics in Twitter\" by L. M. Aiello et al., IEEE Transactions on Multimedia, vol. 15, no. 6, pp. 1268-1282, Oct. 2013, doi: 10.1109/TMM.2013.2265080.\n",
    "\n",
    "•\tThe suggested Databricks runtime version is the 12.2 LTS ML (Scala 2.12, Spark 3.3.2) instead the Standard version.\n",
    "\n",
    "•\tYou have to follow the instructions provided in this webpage to install the SparkNLP library in Databricks, in order to be able to use it in that environment:\n",
    "\n",
    "https://sparknlp.org/docs/en/install#databricks-support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56fe6d79-50c8-4146-99a9-4f316d1e738a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## STUDENTS IDENTIFICATION\n",
    "100548074 - Denilson Hernandez Diaz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a531a1da-ec78-4dcd-a254-7309685c8ed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Common code\n",
    "The first part of the notebook is a skeleton, with the code to fetch the data from the server and start to analize the data provided and pre-process parts of it to fit the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Set Up Environment on Terminal (before running on Jupyter Notebook): \n",
    "(a)\n",
    "I'm running Java 11 (for better compatibility)\n",
    "\n",
    "(b)\n",
    "conda create -n sparknlp python=3.10 -y\n",
    "conda activate sparknlp\n",
    "\n",
    "(c)\n",
    "pip install spark-nlp==6.3.0 pyspark==3.5.0\n",
    "\n",
    "(d)\n",
    "jupyter notebook \n",
    "\n",
    "and then run the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/31 15:25:21 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "# This automatically pulls spark-nlp-silicon-6.3.0.jar if pip version is 6.3.0\n",
    "spark = sparknlp.start(apple_silicon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't show warnings\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8G\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp-silicon_2.12:6.3.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c11772-3d13-4056-a1da-13bfdabafc78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/denilson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/denilson/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/denilson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/denilson/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler, Pipeline\n",
    "from sparknlp.annotator import LanguageDetectorDL\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import TimestampType, StringType\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, NGram,  CountVectorizer, IDF\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')          \n",
    "nltk.download('omw-1.4')            \n",
    "nltk.download('stopwords')          \n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41896dd9-81bf-454e-9fcb-16fbf5dc2559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, explode, from_json\n",
    "from pyspark.sql.types import *\n",
    "import sparknlp\n",
    "import json\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4af0101e-e86a-4f39-a953-1b96d5eb804b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import LanguageDetectorDL\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd3c169b-b5fd-4167-9301-c6442764cb5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Link used to get data\n",
    "datafile=\"http://mcomputing.tsc.uc3m.es/get_tweets.php?start_year=2019&start_month=08&start_day=01&start_hour=02&start_minute=00&minutes_length=20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c90ce26-a0de-49f4-ace8-6c86da0ba45f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with request.urlopen(datafile) as url:\n",
    "    data=url.read().decode()\n",
    "    with open(\"/tmp/temporal.json\",\"w\") as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20ec17e5-1708-4153-950c-be9c966ac4c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "tweetDF= spark.read.json(\"file:///tmp/temporal.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630441be-c2d0-4425-94bd-3ec4b92f45ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#time is UTC -6 (Florida/ Eastern time)\n",
    "#display(tweetDF.select(\"created_at\",\"id\",\"user.screen_name\",\"text\").head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d47ebf1-4187-48fe-8d18-15092f41cfda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44412"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b38b920c-68a6-47b5-8e52-491941653661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filteredDF=tweetDF.select(\"created_at\",\"user.id\",\"user.name\",\"user.screen_name\",\"user.lang\",\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2a1ea58-864a-4e57-b5b3-d8946a7b6f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(filteredDF.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c33f079-a90f-469c-afb0-b59a3bf2e645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#convert doc version of our text\n",
    "document_assembler = (\n",
    "    DocumentAssembler()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"document\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f69c1e4a-2159-4cff-9f81-f8bed9ced4c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ld_wiki_tatoeba_cnn_21 download started this may take some time.\n",
      "Approximate size to download 7.1 MB\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/31 15:28:08 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n",
      "25/12/31 15:28:09 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ld_wiki_tatoeba_cnn_21 download started this may take some time.\n",
      "Approximate size to download 7.1 MB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 15:28:12.744374: W external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/anaconda3/envs/sparknlp/lib/python3.10/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar) to field java.lang.ref.Reference.referent\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "#detects language\n",
    "languageDetector = (\n",
    "    LanguageDetectorDL.pretrained()\n",
    "    .setInputCols(\"document\")\n",
    "    .setOutputCol(\"language\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5467b39-ec7e-4120-b158-d441dcc40de7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nlpPipeline = Pipeline(stages=[document_assembler, languageDetector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50292718-c581-4155-8a5c-090bdb222cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = nlpPipeline.fit(filteredDF).transform(filteredDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c404e43-7d02-4139-bb88-5a281a4b4dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def filterLang(data):\n",
    "    result=False\n",
    "    result = 'en' in data\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d58702c-5bc8-4a8c-9cdb-596cdf22bd12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "udf_filterLang = udf(filterLang,BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5caa1c04-f43b-450d-b290-67ebc0e46b63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataFilteredDF=result.filter(udf_filterLang(\"language.result\")).select(\"created_at\",\"text\",\"language.result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad4ee52-a090-4590-8b76-2294b14d180a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(created_at='Thu Aug 01 08:00:00 +0000 2019', text='kamala harris fucked up bgt parah', result=['en']),\n",
       " Row(created_at='Thu Aug 01 08:00:00 +0000 2019', text='RT @jnkchanels: this is so sad https://t.co/oQr6apodZ1', result=['en'])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataFilteredDF.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c6c4810-2ac2-4ab8-b928-8a1470828a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### MY WORK STARTS HERE\n",
    "\n",
    "In this portion, we clean up the twitter data further \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7a61c4-5dec-4311-8155-321ee7e19194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c174fc26-58f6-4e21-a433-656faac74e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Here I change the format of the timestamp to something easier to work with. \n",
    "df_parsed = dataFilteredDF.withColumn(\"parsed_timestamp\", F.to_timestamp(F.col(\"created_at\"), \"EEE MMM dd HH:mm:ss ZZZZZ yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43485cb7-429d-4573-8d22-057b54a7b213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Here we remove usernames, webesite links, punctuation, and tokenize tweets. \n",
    "#We also lemmatize the data and delete som common words that have no meaning for topics. \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    return \n",
    "\n",
    "def lemmatization(tokens):\n",
    "    if tokens is None:\n",
    "        return None\n",
    "    return [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    no_ids = re.sub(r'@\\w+', '', text)\n",
    "    no_links = re.sub(r'https?://\\S+', '', no_ids)\n",
    "    no_punc = re.sub(r\"[^\\w\\s]\", \" \", no_links)\n",
    "    lower_text = no_punc.lower()\n",
    "    tokens = lower_text.split()\n",
    "    return tokens\n",
    "  \n",
    "delete_rt = set([\"rt\", \"re\", \"m\", \"reply\"])\n",
    "def remove_rt(tokens):\n",
    "    return [t for t in tokens if t not in delete_rt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32c2e32a-12b2-4a84-bf7b-d66eb0ca8f20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_rdd = df_parsed.select(\"parsed_timestamp\", \"text\").rdd.map(lambda row: (row[0], row[1] if row[1] else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa74237d-8b4e-4f9f-b83e-14c23410073b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd_tokens_no_stop = df_rdd.map(\n",
    "    lambda x: (\n",
    "        x[0],\n",
    "        lemmatization(remove_rt(clean_and_tokenize(x[1])))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d4d613-a3f2-4814-a667-34392ceed0f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), words=['kamala', 'harris', 'fucked', 'up', 'bgt', 'parah']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), words=['this', 'is', 'so', 'sad']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), words=['ako', 'sana', 'kaya', 'lang', 'walang', 'pera', 'wait', 'hanap', 'buhay', 'muna', 'ako']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), words=['amen']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), words=['nerf', 'everywhere'])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_clean_tokenized = rdd_tokens_no_stop.toDF([\"created_at\", \"words\"])\n",
    "df_clean_tokenized.cache()\n",
    "display(df_clean_tokenized.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231f4f71-7c29-404e-9586-53ac152cb2a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), words=['kamala', 'harris', 'fucked', 'up', 'bgt', 'parah']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), words=['this', 'is', 'so', 'sad']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), words=['ako', 'sana', 'kaya', 'lang', 'walang', 'pera', 'wait', 'hanap', 'buhay', 'muna', 'ako']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), words=['amen']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), words=['nerf', 'everywhere'])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We define a helper function to extract data or tweets only writtten in English or using the Latin-script alphabet.\n",
    "def all_english(tokens):\n",
    "    for token in tokens:\n",
    "        if not re.match(r'^[a-zA-Z]+$', token):\n",
    "            return False\n",
    "    return True\n",
    "  \n",
    "all_english_udf = udf(all_english, BooleanType())\n",
    "\n",
    "df_clean_tokenized = df_clean_tokenized.filter(all_english_udf(df_clean_tokenized[\"words\"]))\n",
    "df_clean_tokenized.cache()\n",
    "display(df_clean_tokenized.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3062a1e8-d21b-4528-9c33-d236df7ece28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "df_filtered = remover.transform(df_clean_tokenized)[\"created_at\", \"filtered_words\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f34422-6f97-4b94-b93d-3fc1601bfa83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['kamala', 'harris', 'fucked', 'bgt', 'parah']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['sad']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['ako', 'sana', 'kaya', 'lang', 'walang', 'pera', 'wait', 'hanap', 'buhay', 'muna', 'ako']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['amen']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['nerf', 'everywhere'])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_filtered.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e550008-0a1c-43df-8ed8-01bec64bb141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[created_at: timestamp, filtered_words: array<string>, raw_features: vector]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute tf_idf score\n",
    "cv = CountVectorizer(\n",
    "    inputCol=\"filtered_words\", \n",
    "    outputCol=\"raw_features\",\n",
    "    vocabSize=2**16, \n",
    "    minDF=2          \n",
    ")\n",
    "cv_model = cv.fit(df_filtered)\n",
    "df_featurized = cv_model.transform(df_filtered)\n",
    "df_featurized.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "718b734a-ad57-47ac-b301-2abe6a573459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['kamala', 'harris', 'fucked', 'bgt', 'parah'], raw_features=SparseVector(5323, {245: 1.0, 327: 1.0, 849: 1.0, 5018: 1.0})),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['sad'], raw_features=SparseVector(5323, {202: 1.0})),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['ako', 'sana', 'kaya', 'lang', 'walang', 'pera', 'wait', 'hanap', 'buhay', 'muna', 'ako'], raw_features=SparseVector(5323, {129: 1.0, 395: 1.0, 706: 2.0, 1024: 1.0, 1247: 1.0, 2593: 1.0, 3006: 1.0, 4368: 1.0, 4494: 1.0, 4727: 1.0})),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['amen'], raw_features=SparseVector(5323, {772: 1.0})),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['nerf', 'everywhere'], raw_features=SparseVector(5323, {1139: 1.0}))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_featurized.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd5008d-8053-4b04-be7f-da2883a4b43b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[created_at: timestamp, filtered_words: array<string>, raw_features: vector, features: vector]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(df_featurized)\n",
    "df_tfidf = idfModel.transform(df_featurized)\n",
    "df_tfidf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2698077-39df-4c64-8138-79b38f06a97a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['kamala', 'harris', 'fucked', 'bgt', 'parah'], raw_features=SparseVector(5323, {245: 1.0, 327: 1.0, 849: 1.0, 5018: 1.0}), features=SparseVector(5323, {245: 5.5148, 327: 5.731, 849: 6.5333, 5018: 7.9997})),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['sad'], raw_features=SparseVector(5323, {202: 1.0}), features=SparseVector(5323, {202: 5.4607})),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['ako', 'sana', 'kaya', 'lang', 'walang', 'pera', 'wait', 'hanap', 'buhay', 'muna', 'ako'], raw_features=SparseVector(5323, {129: 1.0, 395: 1.0, 706: 2.0, 1024: 1.0, 1247: 1.0, 2593: 1.0, 3006: 1.0, 4368: 1.0, 4494: 1.0, 4727: 1.0}), features=SparseVector(5323, {129: 5.1093, 395: 5.8402, 706: 13.0667, 1024: 6.7004, 1247: 7.0188, 2593: 7.4889, 3006: 7.712, 4368: 7.9997, 4494: 7.9997, 4727: 7.9997})),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['amen'], raw_features=SparseVector(5323, {772: 1.0}), features=SparseVector(5323, {772: 6.4592})),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['nerf', 'everywhere'], raw_features=SparseVector(5323, {1139: 1.0}), features=SparseVector(5323, {1139: 6.7957}))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_tfidf.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7c076c9-d6f4-4559-a4a1-1ac1d4d515b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5323\n"
     ]
    }
   ],
   "source": [
    "idf_values = idfModel.idf.toArray() \n",
    "print(len(idf_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "083b531d-25fe-4c59-9c19-9280f7422252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5323\n"
     ]
    }
   ],
   "source": [
    "#Vocabulary extracted from the data\n",
    "vocabulary = cv_model.vocabulary\n",
    "print(\"Vocabulary size:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3107dee7-6696-439a-b068-0db2d0c212f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', np.float64(3.087023693763398)),\n",
       " ('one', np.float64(3.3052772597834164)),\n",
       " ('wa', np.float64(3.3945083935113587)),\n",
       " ('u', np.float64(3.42840994518704)),\n",
       " ('love', np.float64(3.503579488565721)),\n",
       " ('get', np.float64(3.5110422097673104)),\n",
       " ('people', np.float64(3.597032657622833)),\n",
       " ('good', np.float64(3.6052294248270114)),\n",
       " ('know', np.float64(3.668945239213119)),\n",
       " ('time', np.float64(3.7183935146271003))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we print the top 10 words based on their global tf-idf score. \n",
    "#A lower scores indicate a higher frequency of a word appearing. \n",
    "#This can be used for camparison to compare the top words and topics. \n",
    "\n",
    "word_idf_pairs = list(zip(vocabulary, idf_values))\n",
    "sorted_word_idf = sorted(word_idf_pairs, key=lambda x: x[1])\n",
    "sorted_word_idf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "376888f7-702d-4683-b63f-96d94a0c7023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['kamala', 'harris', 'fucked', 'bgt', 'parah']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['sad']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['ako', 'sana', 'kaya', 'lang', 'walang', 'pera', 'wait', 'hanap', 'buhay', 'muna', 'ako']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['amen']),\n",
       " Row(created_at=datetime.datetime(2019, 8, 1, 1, 0), filtered_words=['nerf', 'everywhere'])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This is the resulting data frame that will be used to pass into our algorithm for topic detection \n",
    "df_algorithm = df_tfidf[\"created_at\", \"filtered_words\"]\n",
    "df_algorithm.cache()\n",
    "display(df_algorithm.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('created_at', 'timestamp'), ('filtered_words', 'array<string>')]\n"
     ]
    }
   ],
   "source": [
    "print(df_algorithm.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0e9c3c-ca41-4634-b542-d61b49233283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### BN Grams Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db47f08c-c14f-475a-bb5c-dde96ae64d6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### HELPER FUNCTIONS\n",
    "def overlap(ng1, ng2):   \n",
    "      s1 = set(ng1.split())\n",
    "      s2 = set(ng2.split())\n",
    "      return len(s1.intersection(s2)) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c389d34-9498-4565-840b-b7d3d9f5784b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Here we define our report function\n",
    "def detect_bn_gram_topics(df, start_time, end_time, top_k=100, num_topics=10, ngram_size=2):  \n",
    "    \"\"\"\n",
    "    Detect BN-gram topics (n-grams) in tweets from df that fall into [start_time, end_time).\n",
    "\n",
    "    df: Spark DataFrame. Must have columns:\n",
    "            - created_at TimestampType  \n",
    "            - filtered_words already tokenized & cleaned\n",
    "    start_time, end_time: The time window for filtering.\n",
    "    top_k: Number of top n-grams to consider by frequency.\n",
    "    num_topics: How many final topics to return after ranking.\n",
    "    ngram_size: size of n-grams (2 for bigrams, 3 for trigrams, etc.)\n",
    "    return: A list of (topic_ngrams, topic_score).\n",
    "    \"\"\"\n",
    "\n",
    "    #Here we filter the data frame a time window \n",
    "    df_window = df.filter(\n",
    "        (F.col(\"created_at\") >= F.lit(start_time)) &\n",
    "        (F.col(\"created_at\") < F.lit(end_time))\n",
    "    )\n",
    "    #Return and empty list if the time window contains nothing \n",
    "    if df_window.count() == 0:\n",
    "        return []\n",
    "    \n",
    "    #Generate NGrams    \n",
    "    ngram = NGram(n=ngram_size, inputCol=\"filtered_words\", outputCol=\"ngrams\")\n",
    "    df_ngrams = ngram.transform(df_window)\n",
    "\n",
    "    #Explode and Count N-Grams\n",
    "    df_ngrams_exploded = df_ngrams.select(F.explode(\"ngrams\").alias(\"ngram\"))\n",
    "    df_ngrams_freq = df_ngrams_exploded.groupBy(\"ngram\").count().withColumnRenamed(\"count\", \"current_count\")\n",
    "    total_current = df_ngrams_exploded.count()\n",
    "\n",
    "    #Select the top K candidates \n",
    "    df_ratio = df_ngrams_freq\n",
    "\n",
    "    df_candidates = df_ratio.orderBy(F.desc(\"current_count\")).limit(top_k).dropDuplicates([\"ngram\"])\n",
    "    ratio_map = {row.ngram: row[\"current_count\"] for row in df_candidates.collect()}  # re-use as \"ratio\"\n",
    "    freq_map  = ratio_map\n",
    "\n",
    "    candidate_ngrams = list(ratio_map.keys())\n",
    "\n",
    "    #Build topics by overlap. Group similar n-grams together to form [topics].\n",
    "    topics = []\n",
    "    for ng in candidate_ngrams:\n",
    "        placed = False\n",
    "        for t in topics:\n",
    "            #if overlap with any ngram in topic t, add it\n",
    "            if any(overlap(ng, existing_ng) for existing_ng in t):\n",
    "                t.append(ng)\n",
    "                placed = True\n",
    "                break\n",
    "        if not placed:\n",
    "            topics.append([ng])\n",
    "\n",
    "    #Score each topic and sort. \n",
    "    #Sum up all ng scores (ratio_map[ng] = current_count).\n",
    "    #Divide by len(t) to get an average frequency (or “score”) of that topic.\n",
    "    scored_topics = []\n",
    "    for t in topics:\n",
    "        if t:\n",
    "            #Compute ratio\n",
    "            s = sum(ratio_map[ng] for ng in t) / len(t)\n",
    "        else:\n",
    "            s = 0\n",
    "        scored_topics.append((t, s))\n",
    "\n",
    "    scored_topics.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return top N topics \n",
    "    final_topics = scored_topics[:num_topics]\n",
    "\n",
    "    return final_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41fbff14-42dd-4cc6-bb32-781c32b6ff2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Processing window: 2019-08-01 01:00:00 - 2019-08-01 01:05:00 ####\n",
      "Number of topics returned: 10\n",
      "  Topic 1: ['biggest fan week'], Score=10.000\n",
      "  Topic 2: ['malay kan english'], Score=6.000\n",
      "  Topic 3: ['suspended suspended suspended'], Score=5.000\n",
      "  Topic 4: ['stay cool everyone', 'cool everyone icecream', 'whatsapp hookup cool'], Score=4.400\n",
      "  Topic 5: ['recognition camera cyber', 'facial recognition camera', 'camera cyber war'], Score=4.000\n",
      "  Topic 6: ['protestors another level', 'hong kong protestors', 'kong protestors another'], Score=3.889\n",
      "  Topic 7: ['weversetrans v op', 'v became small'], Score=3.500\n",
      "  Topic 8: ['kaorhys angaugustoko kaorhys', 'angaugustoko kaorhys angaugustoko'], Score=3.500\n",
      "  Topic 9: ['sco pa tu', 'pa tu manaa', 'tu manaa hell'], Score=3.455\n",
      "  Topic 10: ['happy new month', 'happy national girlfriend'], Score=3.000\n",
      "\n",
      "#### Processing window: 2019-08-01 01:05:00 - 2019-08-01 01:10:00 ####\n",
      "Number of topics returned: 10\n",
      "  Topic 1: ['stay cool everyone', 'cool everyone icecream'], Score=7.000\n",
      "  Topic 2: ['happy new month'], Score=7.000\n",
      "  Topic 3: ['fuck anxiety fuck', 'anxiety fuck anxiety'], Score=5.000\n",
      "  Topic 4: ['august filled blessing', 'filled blessing august', 'blessing august filled'], Score=4.429\n",
      "  Topic 5: ['ashtan isourchoice ashtan', 'isourchoice ashtan isourchoice'], Score=4.000\n",
      "  Topic 6: ['police killed tony', 'dallas police killed'], Score=4.000\n",
      "  Topic 7: ['national girlfriend day'], Score=4.000\n",
      "  Topic 8: ['link patch note'], Score=4.000\n",
      "  Topic 9: ['mentally ill man', 'man called help', 'tony timpa mentally'], Score=3.750\n",
      "  Topic 10: ['sco pa tu', 'pa tu manaa', 'tu manaa hell'], Score=3.714\n",
      "\n",
      "#### Processing window: 2019-08-01 01:10:00 - 2019-08-01 01:15:00 ####\n",
      "Number of topics returned: 10\n",
      "  Topic 1: ['stay cool everyone', 'cool everyone icecream'], Score=9.000\n",
      "  Topic 2: ['happy new month', 'happy national girlfriend'], Score=5.000\n",
      "  Topic 3: ['stop giving money', 'family stop giving', 'hate family stop'], Score=5.000\n",
      "  Topic 4: ['weversetrans jin op'], Score=5.000\n",
      "  Topic 5: ['sco pa tu', 'pa tu manaa', 'tu manaa hell'], Score=4.286\n",
      "  Topic 6: ['leave twitter go', 'y leave twitter'], Score=4.000\n",
      "  Topic 7: ['another app called'], Score=4.000\n",
      "  Topic 8: ['get older like', 'older like need', 'birthday get older'], Score=3.625\n",
      "  Topic 9: ['season x battle', 'x battle pas', 'battle pas available'], Score=3.450\n",
      "  Topic 10: ['third bowl ice', 'bowl ice cream', 'ice cream fat'], Score=3.000\n",
      "\n",
      "#### Processing window: 2019-08-01 01:15:00 - 2019-08-01 01:20:00 ####\n",
      "Number of topics returned: 10\n",
      "  Topic 1: ['national girlfriend day'], Score=7.000\n",
      "  Topic 2: ['gt gt gt'], Score=6.000\n",
      "  Topic 3: ['get joke subject', 'gotta get joke', 'chat lit gotta'], Score=5.000\n",
      "  Topic 4: ['group chat lit'], Score=5.000\n",
      "  Topic 5: ['stay cool everyone', 'cool everyone icecream', 'cool cool cool'], Score=4.333\n",
      "  Topic 6: ['followed automatically checked', 'person followed automatically', 'one person followed'], Score=4.000\n",
      "  Topic 7: ['murid dia tapi', 'memang sikit murid', 'tapi masing masing'], Score=4.000\n",
      "  Topic 8: ['perhatian khusus tak', 'diberi perhatian khusus', 'khusus tak boleh'], Score=4.000\n",
      "  Topic 9: ['khas memang sikit'], Score=4.000\n",
      "  Topic 10: ['keperluan yang perlu'], Score=4.000\n"
     ]
    }
   ],
   "source": [
    "#Function that cycles over different time windows to extract trending topics\n",
    "\n",
    "# Here we define the starting time window, the desired window time length to process, and define the end time of the overall tweet data.\n",
    "'''\n",
    "start_time_dt: Year, month, day, hour, minute, second. This sets the starting time. \n",
    "               This must be the starting time of all the data, taking into account a specific date.\n",
    "window_length: Length of time to analyze per loop.\n",
    "end_time_dt: End time of tweet data. \n",
    "'''\n",
    "start_time_dt = datetime(2019, 8, 1, 1, 0, 0)\n",
    "window_length = 5 \n",
    "end_time_dt   = start_time_dt + timedelta(minutes= 20) \n",
    "\n",
    "current_window_start = start_time_dt\n",
    "\n",
    "while current_window_start < end_time_dt:\n",
    "    current_window_end = current_window_start + timedelta(minutes=window_length)\n",
    "    if current_window_end > end_time_dt:\n",
    "        current_window_end = end_time_dt\n",
    "    \n",
    "    #Convert timesatmps to string to print out the report \n",
    "    window_start_str = current_window_start.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    window_end_str   = current_window_end.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    print(f\"\\n#### Processing window: {window_start_str} - {window_end_str} ####\")\n",
    "    #Call the topic detection function\n",
    "    topics = detect_bn_gram_topics(\n",
    "        df_algorithm,\n",
    "        start_time= window_start_str,\n",
    "        end_time= window_end_str,\n",
    "        top_k=150,\n",
    "        num_topics=10,\n",
    "        ngram_size=3\n",
    "    )\n",
    "    print(\"Number of topics returned:\", len(topics))\n",
    "\n",
    "    for i, (t_bigrams, score) in enumerate(topics):\n",
    "        print(f\"  Topic {i+1}: {t_bigrams[:3]}, Score={score:.3f}\")\n",
    "\n",
    "    # Advance the window\n",
    "    current_window_start = current_window_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7498f09b-7ff1-4018-9ce0-435df6e8b091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### CONCLUSIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee0010bd-f885-410e-80dd-e74ad6f3d456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this project, I implemented the BN-Grams algorithm described in the article “Sensing Trending Topics in Twitter” by L. M. Aiello.\n",
    "\n",
    "### IMPLEMENTATION\n",
    "My first steps centered on cleaning the tweet data. In class, I filtered the tweets so only English ones remained for topic detection. Although this tool was effective, some non-English tweets still got through.\n",
    "\n",
    "Next, I cleaned the data further by removing any tweets that contained characters outside of the English alphabet. I performed this after removing usernames, links, punctuation, tokenizing, and using a WordNetLemmatizer to lemmatize the English words. This helped identify tweets in other scripts, but some using Latin letters (like Malay) still slipped in.\n",
    "\n",
    "Following Aiello’s suggestions, I encountered a few issues with the lemmatizer because some words became ambiguous, like “wa,” and others got shortened in hard-to-read ways. I also used a stop word remover to further prepare the data.\n",
    "\n",
    "After cleaning, I computed the global tf-idf score for each word in the tweets’ vocabulary. These scores could help compare the results of trending topics and their frequency. The DataFrame passed to the function is called df_algorithm.\n",
    "\n",
    "I chose to implement BN-Grams for topic extraction. My DataFrame contained the columns “created_at” (the tweet’s timestamp in a more workable format) and “filtered_words” (the tokenized tweets).\n",
    "\n",
    "In my report function, I focused on trending topics over a specific time interval. I filtered tweets to a window [start_time, end_time), ensuring only recent tweets were considered. I used Spark’s NGram transformer to produce bigrams or trigrams. By setting ngram_size=3, each tweet was converted to consecutive three-word phrases. This can be changed by the user to find different n-gram lengths. I used three to preserve short but meaningful units that might point to a possible topic. Single-word n-grams may not capture meaningful phrases.\n",
    "\n",
    "Once the trigrams were generated, I exploded them into rows and grouped by the trigram string to compute a simple frequency (current_count) using SQL-like functions. Then I sorted them by descending frequency and limited the list to top_K (150). This step kept the focus on the most frequent trigrams rather than rare noise. Some clusters contained many n-grams, so in the final report function I only chose up to three topics in each cluster of related n-grams. This variable is also user-configurable. Decreasing it reduces noise in potential topics, while increasing it yields more possible topics. I set it to 150 to ensure finding at least 10 clusters of possible topics. When I used a smaller number, the algorithm sometimes couldn’t find 10 possible “topics.”\n",
    "\n",
    "The algorithm grouped trigrams that overlapped by at least one word. For instance, if “facial recognition camera” shares “camera” with “camera cyber war,” they would be merged into one topic. Finally, each topic was assigned a score based on the average frequency (or ratio) of its trigrams. The top N clusters (in my case, the top 10) became the final “trending topics.”\n",
    "\n",
    "I chose BN-Grams because it captures short yet semantically relevant phrases without too much data sparsity, and it integrates easily with Spark’s native NGram and DataFrame operations.\n",
    "\n",
    "### PROBLEMS I ENCOUNTERED\n",
    "Lemmatization and removing non-English tweets was difficult. Some trending topics in the final results are in other languages that also use a Latin script.\n",
    "\n",
    "One issue was that too many bigrams or trigrams ended up in a single large cluster. If trigram A shared a word with B, and B shared a different word with C, they would all merge, creating big clusters for “final topics.” That’s why I decided to show only up to three topics in each cluster.\n",
    "\n",
    "Another challenge was handling repeated words, like “hello hello hello,” or merging trigrams that caused the same word to appear multiple times. I tried implementing a fix to detect these patterns and discard them, but it wasn’t successful. Some trigrams are duplicates because of data duplication, so I used dropDuplicates to handle that.\n",
    "\n",
    "While completing this project, I broke my left arm. I had to undergo two bone fracture reductions and couldn’t communicate much with the professor, which limited the time I could spend on the project and understading of some aspects of Natural Language Processing.\n",
    "\n",
    "### PERFORMANCE AND RESULTS\n",
    "The time window length is user-adjustable. The larger the window, the longer each cleaning step takes. The longest window I tried was 60 minutes, and I’m not entirely sure how to speed things up beyond caching intermediate results.\n",
    "\n",
    "The results for each topic show clusters, with each phrase made up of three words that share some relationship. Some topics appear in different languages that use a Latin script.\n",
    "\n",
    "### MORE GENERAL CONCLUSIONS AND LESSONS LEARNED\n",
    "Implementing BN-Grams showed me how powerful Apache Spark can be for handling large datasets and text mining. Trigrams capture more context than single words yet are still common enough to be meaningful. However, bigrams were also useful. For example, when I used ngram_size=2, “kamala harris” came up along with other trending topics about U.S. presidential debates. I ultimately chose trigrams in my final setup, but it’s flexible for the user.\n",
    "\n",
    "Simple overlap checks can lead to chain merges, so tuning merge conditions or limiting how many trigrams you consider can be tricky but is important for coherent topics. Repeated words in n-grams can inflate certain topics, and while I tried to fix the merging logic, it remained challenging.\n",
    "\n",
    "Because BN-Grams integrates well with Spark’s DataFrame operations, it can scale to large tweet datasets. Overall, BN-Grams strikes a nice balance between capturing enough context (multi-word phrases) and staying computationally feasible.\n",
    "\n",
    "This was an interesting project, though I didn’t feel I had enough NLP theory to fully interpret the results. Nevertheless, it highlights how useful Apache Spark can be for large-scale text processing."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Final_2024_DenilsonHernandez",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
